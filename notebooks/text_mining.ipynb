{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6c4e08",
   "metadata": {},
   "source": [
    "# News Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19054be7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d60bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // TODO: TINGS\n",
    "#     ✓ Find API for data collection\n",
    "#     - GPU Selector\n",
    "#     - Data Gathering\n",
    "#     - Data Cleaning\n",
    "#     - Feature generation\n",
    "#     - Feature Engineering/selection\n",
    "#     - Model Train\n",
    "#     - Model Test\n",
    "#     - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0542719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Cleaner output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "\n",
    "# Add the path to the API Scraper\n",
    "## Project Path\n",
    "project_path = \"../\"\n",
    "\n",
    "## Add the path to API Scraper\n",
    "sys.path.append(os.path.abspath(os.path.join(project_path, \"lib\")))\n",
    "\n",
    "# Custom API Scraping Libraries\n",
    "from scraper import get_cached_news_metadata, extract_text_from_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70acbf",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ea1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(n_pages: int = 10, before_date: str = \"2025-12\", path: str = \".\") -> pd.DataFrame:\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for i in tqdm(range(n_pages), desc=\"Fetching News Data...\", unit=\"news\"):\n",
    "        metadata = get_cached_news_metadata(page=i, before_date=before_date, path=path)\n",
    "\n",
    "        data_list = metadata.get(\"data\", [])\n",
    "\n",
    "        for article in data_list:\n",
    "            # extract sentiment (first entity if exists)\n",
    "            entities = article.get(\"entities\", [])\n",
    "            if entities and \"sentiment_score\" in entities[0]:\n",
    "                sentiment = entities[0][\"sentiment_score\"]\n",
    "            else:\n",
    "                sentiment = None\n",
    "\n",
    "            # store sentiment as its own field inside article\n",
    "            article[\"sentiment\"] = sentiment\n",
    "\n",
    "            articles.append(article)\n",
    "\n",
    "    # final dataframe\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d055a6f",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd5b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n",
      "Cached dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# caching the dataset\n",
    "before_date = \"2025-12\"\n",
    "\n",
    "data_path = os.path.join(project_path,f\"news_cache/{before_date}/csv/\")\n",
    "os.makedirs(data_path, exist_ok=True) # if the directory exist no need to make\n",
    "\n",
    "cached_file = os.path.join(data_path, f\"{before_date}_news_data.csv\")\n",
    "\n",
    "get_new_data = False\n",
    "\n",
    "# We will cache the data so that it will load faster\n",
    "if os.path.exists(cached_file) and not get_new_data:\n",
    "    print(\"Loading cached dataset...\")\n",
    "    news_df = pd.read_csv(cached_file)\n",
    "    print(\"Cached dataset loaded\")\n",
    "\n",
    "elif os.path.exists(cached_file) and get_new_data:\n",
    "    print(\"Overwriting old data and caching new data...\")\n",
    "    news_df = scrape_data(n_pages=95, before_date= before_date, path= project_path)\n",
    "    news_df.to_csv(cached_file, index=False)\n",
    "    print(\"Done Overwriting old data and caching new data...\")\n",
    "\n",
    "else:\n",
    "    print(\"Creating and caching dataset...\")\n",
    "    news_df = scrape_data(n_pages=95, before_date= before_date, path= project_path)\n",
    "    news_df.to_csv(cached_file, index=False)\n",
    "    print(\"Finished Caching\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc09d2",
   "metadata": {},
   "source": [
    "### Fetch the Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a3da35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.manilatimes.net/2025/11/30/tmt-newswire/globenewswire/airlife-gases-acquires-control-of-royal-helium-ltd/2233736 on URL https://www.manilatimes.net/2025/11/30/tmt-newswire/globenewswire/airlife-gases-acquires-control-of-royal-helium-ltd/2233736",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArticleException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m news_df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mnews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_text_from_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mextract_text_from_url\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      8\u001b[39m article = Article(url)\n\u001b[32m      9\u001b[39m article.download()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43marticle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m article.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/newspaper/article.py:191\u001b[39m, in \u001b[36mArticle.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc = \u001b[38;5;28mself\u001b[39m.config.get_parser().fromstring(\u001b[38;5;28mself\u001b[39m.html)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m.clean_doc = copy.deepcopy(\u001b[38;5;28mself\u001b[39m.doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/text_mining_research-env/lib/python3.11/site-packages/newspaper/article.py:531\u001b[39m, in \u001b[36mArticle.throw_if_not_downloaded_verbose\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[33m'\u001b[39m\u001b[33mYou must `download()` an article first!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_state == ArticleDownloadState.FAILED_RESPONSE:\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[33m'\u001b[39m\u001b[33mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m %\n\u001b[32m    532\u001b[39m           (\u001b[38;5;28mself\u001b[39m.download_exception_msg, \u001b[38;5;28mself\u001b[39m.url))\n",
      "\u001b[31mArticleException\u001b[39m: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.manilatimes.net/2025/11/30/tmt-newswire/globenewswire/airlife-gases-acquires-control-of-royal-helium-ltd/2233736 on URL https://www.manilatimes.net/2025/11/30/tmt-newswire/globenewswire/airlife-gases-acquires-control-of-royal-helium-ltd/2233736"
     ]
    }
   ],
   "source": [
    "news_df[\"text\"] = news_df[\"url\"].apply(extract_text_from_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdda819",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d50d2",
   "metadata": {},
   "source": [
    "### View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a066aab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "uuid",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "snippet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "relevance_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "entities",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "similar",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e1a7ab29-edd5-4c13-974b-4c5ff15c1d46",
       "rows": [
        [
         "0",
         "487e6a88-d3c2-4ae1-8dc2-26af6b31d688",
         "2025: The Year Of Alphabet (GOOG)",
         "No stock has seen a bigger jump recently than Alphabet, whose market cap has risen by more than half of a trillion dollars since 10/29.",
         null,
         "vzphotos/iStock Editorial via Getty Images\n\nSince the Nasdaq 100 peaked on 10/29, the 25 largest stocks in the index have lost roughly $1 trillion in market cap...",
         "https://seekingalpha.com/article/4848680-2025-year-of-alphabet",
         "https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1092952108/image_1092952108.jpg?io=getty-c-w1536",
         "en",
         "2025-11-30T05:30:00.000000Z",
         "seekingalpha.com",
         null,
         "[{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', 'exchange': None, 'exchange_long': None, 'country': 'us', 'type': 'equity', 'industry': 'Communication Services', 'match_score': 39.62733, 'sentiment_score': 0, 'highlights': [{'highlight': '2025: The Year Of <em>Alphabet</em> (GOOG)', 'sentiment': 0, 'highlighted_in': 'title'}]}]",
         "[]",
         "0.0"
        ],
        [
         "1",
         "92b5c2bd-d324-4ae8-b115-2cfd95a8fa98",
         "Why I'm Doubling Down On My Adobe Position (NASDAQ:ADBE)",
         "Adobe's revenue is highly predictable, driven by subscription-based Digital Media and Digital Experience segments. Read why ADBE stock is a Strong Buy.",
         null,
         "To say that Adobe ( ADBE ) stock has not had a good year is an understatement. The stock has tanked almost 30% year-to-date , while some of its tech peers, such...",
         "https://seekingalpha.com/article/4848762-why-i-am-doubling-down-on-my-adobe-position",
         "https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1581810849/image_1581810849.jpg?io=getty-c-w1536",
         "en",
         "2025-11-30T05:25:01.000000Z",
         "seekingalpha.com",
         null,
         "[{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exchange': None, 'exchange_long': None, 'country': 'us', 'type': 'equity', 'industry': 'Technology', 'match_score': 39.357193, 'sentiment_score': 0, 'highlights': [{'highlight': \"Why I'm Doubling Down On My Adobe Position (<em>NASDAQ:ADBE</em>)\", 'sentiment': 0, 'highlighted_in': 'title'}]}]",
         "[]",
         "0.0"
        ],
        [
         "2",
         "9084e5f1-75f5-4f15-aa3d-0676073b4aaf",
         "Global week ahead: The start of a Santa Rally or more 'bah humbug'?",
         null,
         "STOXX 600, business news",
         "And just like that... December is upon us. It's been a volatile handover from November with U.S. major indices underperforming, dragged down by steep declines f...",
         "https://www.cnbc.com/2025/11/30/global-week-ahead-december-santa-rally-or-more-bah-humbug.html",
         "https://image.cnbcfm.com/api/v1/image/108232571-17641742322025-11-26t161835z_524706178_rc2d4iayfrpu_rtrmadp_0_usa-stocks.jpeg?v=1764174389&w=1920&h=1080",
         "en",
         "2025-11-30T05:10:58.000000Z",
         "cnbc.com",
         null,
         "[{'symbol': 'M', 'name': \"Macy's, Inc.\", 'exchange': None, 'exchange_long': None, 'country': 'us', 'type': 'equity', 'industry': 'Consumer Cyclical', 'match_score': 14.410284, 'sentiment_score': 0.6908, 'highlights': [{'highlight': \"<em>Macy's</em> Santa Claus is greeted b[+293 characters]\", 'sentiment': 0.6908, 'highlighted_in': 'main_text'}]}]",
         "[]",
         "0.6908"
        ],
        [
         "3",
         "487e6a88-d3c2-4ae1-8dc2-26af6b31d688",
         "2025: The Year Of Alphabet (GOOG)",
         "No stock has seen a bigger jump recently than Alphabet, whose market cap has risen by more than half of a trillion dollars since 10/29.",
         null,
         "vzphotos/iStock Editorial via Getty Images\n\nSince the Nasdaq 100 peaked on 10/29, the 25 largest stocks in the index have lost roughly $1 trillion in market cap...",
         "https://seekingalpha.com/article/4848680-2025-year-of-alphabet",
         "https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1092952108/image_1092952108.jpg?io=getty-c-w1536",
         "en",
         "2025-11-30T05:30:00.000000Z",
         "seekingalpha.com",
         null,
         "[{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', 'exchange': None, 'exchange_long': None, 'country': 'us', 'type': 'equity', 'industry': 'Communication Services', 'match_score': 39.62733, 'sentiment_score': 0, 'highlights': [{'highlight': '2025: The Year Of <em>Alphabet</em> (GOOG)', 'sentiment': 0, 'highlighted_in': 'title'}]}]",
         "[]",
         "0.0"
        ],
        [
         "4",
         "92b5c2bd-d324-4ae8-b115-2cfd95a8fa98",
         "Why I'm Doubling Down On My Adobe Position (NASDAQ:ADBE)",
         "Adobe's revenue is highly predictable, driven by subscription-based Digital Media and Digital Experience segments. Read why ADBE stock is a Strong Buy.",
         null,
         "To say that Adobe ( ADBE ) stock has not had a good year is an understatement. The stock has tanked almost 30% year-to-date , while some of its tech peers, such...",
         "https://seekingalpha.com/article/4848762-why-i-am-doubling-down-on-my-adobe-position",
         "https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1581810849/image_1581810849.jpg?io=getty-c-w1536",
         "en",
         "2025-11-30T05:25:01.000000Z",
         "seekingalpha.com",
         null,
         "[{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exchange': None, 'exchange_long': None, 'country': 'us', 'type': 'equity', 'industry': 'Technology', 'match_score': 39.357193, 'sentiment_score': 0, 'highlights': [{'highlight': \"Why I'm Doubling Down On My Adobe Position (<em>NASDAQ:ADBE</em>)\", 'sentiment': 0, 'highlighted_in': 'title'}]}]",
         "[]",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>snippet</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>language</th>\n",
       "      <th>published_at</th>\n",
       "      <th>source</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>entities</th>\n",
       "      <th>similar</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>487e6a88-d3c2-4ae1-8dc2-26af6b31d688</td>\n",
       "      <td>2025: The Year Of Alphabet (GOOG)</td>\n",
       "      <td>No stock has seen a bigger jump recently than ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vzphotos/iStock Editorial via Getty Images\\n\\n...</td>\n",
       "      <td>https://seekingalpha.com/article/4848680-2025-...</td>\n",
       "      <td>https://static.seekingalpha.com/cdn/s3/uploads...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-11-30T05:30:00.000000Z</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92b5c2bd-d324-4ae8-b115-2cfd95a8fa98</td>\n",
       "      <td>Why I'm Doubling Down On My Adobe Position (NA...</td>\n",
       "      <td>Adobe's revenue is highly predictable, driven ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To say that Adobe ( ADBE ) stock has not had a...</td>\n",
       "      <td>https://seekingalpha.com/article/4848762-why-i...</td>\n",
       "      <td>https://static.seekingalpha.com/cdn/s3/uploads...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-11-30T05:25:01.000000Z</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9084e5f1-75f5-4f15-aa3d-0676073b4aaf</td>\n",
       "      <td>Global week ahead: The start of a Santa Rally ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STOXX 600, business news</td>\n",
       "      <td>And just like that... December is upon us. It'...</td>\n",
       "      <td>https://www.cnbc.com/2025/11/30/global-week-ah...</td>\n",
       "      <td>https://image.cnbcfm.com/api/v1/image/10823257...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-11-30T05:10:58.000000Z</td>\n",
       "      <td>cnbc.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'symbol': 'M', 'name': \"Macy's, Inc.\", 'exch...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>487e6a88-d3c2-4ae1-8dc2-26af6b31d688</td>\n",
       "      <td>2025: The Year Of Alphabet (GOOG)</td>\n",
       "      <td>No stock has seen a bigger jump recently than ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vzphotos/iStock Editorial via Getty Images\\n\\n...</td>\n",
       "      <td>https://seekingalpha.com/article/4848680-2025-...</td>\n",
       "      <td>https://static.seekingalpha.com/cdn/s3/uploads...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-11-30T05:30:00.000000Z</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92b5c2bd-d324-4ae8-b115-2cfd95a8fa98</td>\n",
       "      <td>Why I'm Doubling Down On My Adobe Position (NA...</td>\n",
       "      <td>Adobe's revenue is highly predictable, driven ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To say that Adobe ( ADBE ) stock has not had a...</td>\n",
       "      <td>https://seekingalpha.com/article/4848762-why-i...</td>\n",
       "      <td>https://static.seekingalpha.com/cdn/s3/uploads...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-11-30T05:25:01.000000Z</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  \\\n",
       "0  487e6a88-d3c2-4ae1-8dc2-26af6b31d688   \n",
       "1  92b5c2bd-d324-4ae8-b115-2cfd95a8fa98   \n",
       "2  9084e5f1-75f5-4f15-aa3d-0676073b4aaf   \n",
       "3  487e6a88-d3c2-4ae1-8dc2-26af6b31d688   \n",
       "4  92b5c2bd-d324-4ae8-b115-2cfd95a8fa98   \n",
       "\n",
       "                                               title  \\\n",
       "0                  2025: The Year Of Alphabet (GOOG)   \n",
       "1  Why I'm Doubling Down On My Adobe Position (NA...   \n",
       "2  Global week ahead: The start of a Santa Rally ...   \n",
       "3                  2025: The Year Of Alphabet (GOOG)   \n",
       "4  Why I'm Doubling Down On My Adobe Position (NA...   \n",
       "\n",
       "                                         description  \\\n",
       "0  No stock has seen a bigger jump recently than ...   \n",
       "1  Adobe's revenue is highly predictable, driven ...   \n",
       "2                                                NaN   \n",
       "3  No stock has seen a bigger jump recently than ...   \n",
       "4  Adobe's revenue is highly predictable, driven ...   \n",
       "\n",
       "                   keywords  \\\n",
       "0                       NaN   \n",
       "1                       NaN   \n",
       "2  STOXX 600, business news   \n",
       "3                       NaN   \n",
       "4                       NaN   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  vzphotos/iStock Editorial via Getty Images\\n\\n...   \n",
       "1  To say that Adobe ( ADBE ) stock has not had a...   \n",
       "2  And just like that... December is upon us. It'...   \n",
       "3  vzphotos/iStock Editorial via Getty Images\\n\\n...   \n",
       "4  To say that Adobe ( ADBE ) stock has not had a...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://seekingalpha.com/article/4848680-2025-...   \n",
       "1  https://seekingalpha.com/article/4848762-why-i...   \n",
       "2  https://www.cnbc.com/2025/11/30/global-week-ah...   \n",
       "3  https://seekingalpha.com/article/4848680-2025-...   \n",
       "4  https://seekingalpha.com/article/4848762-why-i...   \n",
       "\n",
       "                                           image_url language  \\\n",
       "0  https://static.seekingalpha.com/cdn/s3/uploads...       en   \n",
       "1  https://static.seekingalpha.com/cdn/s3/uploads...       en   \n",
       "2  https://image.cnbcfm.com/api/v1/image/10823257...       en   \n",
       "3  https://static.seekingalpha.com/cdn/s3/uploads...       en   \n",
       "4  https://static.seekingalpha.com/cdn/s3/uploads...       en   \n",
       "\n",
       "                  published_at            source  relevance_score  \\\n",
       "0  2025-11-30T05:30:00.000000Z  seekingalpha.com              NaN   \n",
       "1  2025-11-30T05:25:01.000000Z  seekingalpha.com              NaN   \n",
       "2  2025-11-30T05:10:58.000000Z          cnbc.com              NaN   \n",
       "3  2025-11-30T05:30:00.000000Z  seekingalpha.com              NaN   \n",
       "4  2025-11-30T05:25:01.000000Z  seekingalpha.com              NaN   \n",
       "\n",
       "                                            entities similar  sentiment  \n",
       "0  [{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', ...      []     0.0000  \n",
       "1  [{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exc...      []     0.0000  \n",
       "2  [{'symbol': 'M', 'name': \"Macy's, Inc.\", 'exch...      []     0.6908  \n",
       "3  [{'symbol': 'GOOGL', 'name': 'Alphabet Inc.', ...      []     0.0000  \n",
       "4  [{'symbol': 'ADBE', 'name': 'Adobe Inc.', 'exc...      []     0.0000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee17f75",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72837a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Number_Missing  Missing_Percentage\n",
      "uuid                          0            0.000000\n",
      "title                         0            0.000000\n",
      "description                   4            1.403509\n",
      "keywords                    131           45.964912\n",
      "snippet                       0            0.000000\n",
      "url                           0            0.000000\n",
      "image_url                     0            0.000000\n",
      "language                      0            0.000000\n",
      "published_at                  0            0.000000\n",
      "source                        0            0.000000\n",
      "relevance_score             285          100.000000\n",
      "entities                      0            0.000000\n",
      "similar                       0            0.000000\n",
      "sentiment                     0            0.000000\n"
     ]
    }
   ],
   "source": [
    "is_na = pd.DataFrame(news_df.isna().sum())\n",
    "is_na.columns = [\"Number_Missing\"]\n",
    "is_na[\"Missing_Percentage\"] = (is_na[\"Number_Missing\"] / len(news_df) * 100)\n",
    "print(is_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf951a",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, language: str='english', tokenize: bool = False, remove_stop_words: bool = False, stem_words: bool = False, remove_url: bool = False, remove_emojis: str = \"convert\", expand_abbreviations: bool=False):\n",
    "    \"\"\"\n",
    "    #### Description:\n",
    "    This function is to clean the text from stopwords, punctuation and return a clean text for further analysis\n",
    "\n",
    "    Args:\n",
    "        text (str):\n",
    "            The dataframe containing the text data\n",
    "        \n",
    "        language (str):\n",
    "            This are the available languages for the stopwords:\n",
    "            - \"catalan\"\n",
    "            - \"czech\"\n",
    "            - \"german\"\n",
    "            - \"greek\"\n",
    "            - \"english\"\n",
    "            - \"spanish\"\n",
    "            - \"finnish\"\n",
    "            - \"french\"\n",
    "            - \"hungarian\"\n",
    "            - \"icelandic\"\n",
    "            - \"italian\"\n",
    "            - \"latvian\"\n",
    "            - \"dutch\"\n",
    "            - \"polish\"\n",
    "            - \"portuguese\"\n",
    "            - \"romanian\"\n",
    "            - \"russian\"\n",
    "            - \"slovak\"\n",
    "            - \"slovenian\"\n",
    "            - \"swedish\"\n",
    "            - \"tamil\"\n",
    "        \n",
    "        tokenize (bool):\n",
    "            True = return tokenized data\n",
    "            False = return untokenized data\n",
    "        \n",
    "        remove_stop_words (bool):\n",
    "            True = remove stop words\n",
    "            False = do not remove stop words\n",
    "\n",
    "        stem_words (bool):\n",
    "            True = get the base words (i.e. spraying -> spray)\n",
    "            False = leave the words as is\n",
    "\n",
    "        remove_url (bool):\n",
    "            True = Remove the url in the text\n",
    "            False = leave the text as is\n",
    "        \n",
    "        remove_emojis (str):\n",
    "            \"remove\" = Removes the emoji in text\n",
    "            \"convert = converts emoji to text (e.g. ❤️ -> :red_heart:)\n",
    "            \"keep\" = keeps the emoji as is\n",
    "        \n",
    "        expand_abbreviations (bool):\n",
    "            True = Expand abbreviations (e.g. brb -> \"be right back\")\n",
    "            False = Keep the abbriviations as is\n",
    "    \"\"\"\n",
    "\n",
    "    # slang dictionary\n",
    "    slang_dict = {\n",
    "        \"brb\": \"be right back\",\n",
    "        \"afk\": \"away from keyboard\",\n",
    "        \"gg\": \"good game\",\n",
    "        \"ggwp\": \"good game well played\",\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"idk\": \"I do not know\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"lmao\": \"laughing my ass off\",\n",
    "        \"lmfao\": \"laughing fucking my ass off\",\n",
    "        \"sus\": \"suspicious\",\n",
    "        \"rekt\": \"wrecked\",\n",
    "        \"noob\": \"new player\",\n",
    "        \"af\": \"as hell\",\n",
    "        \"wtf\": \"what the fuck\",\n",
    "        \"wth\": \"what the heck\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"ty\": \"thank you\",\n",
    "        \"plz\": \"please\",\n",
    "        \"pls\": \"please\",\n",
    "        \"u\": \"you\",\n",
    "        \"r\": \"are\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"fr\": \"for real\",\n",
    "        \"til\": \"today i learned\",\n",
    "        \"asap\": \"as soon as possible\",\n",
    "        \"g2g\": \"got to go\",\n",
    "        \"gtg\": \"got to go\",\n",
    "        \"nc\": \"nice\",\n",
    "        \"fyi\": \"for your information\",\n",
    "        \"ttyl\": \"talk to you later\",\n",
    "        \"fb\": \"facebook\",\n",
    "        \"msg\": \"message\",\n",
    "        \"hifw\": \"how i feel when\",\n",
    "        \"tfw\": \"the feeling when\",\n",
    "        \"mfw\": \"my face when\",\n",
    "        \"mrw\": \"my reaction when\",\n",
    "        \"ifyp\": \"i feel your pain\",\n",
    "        \"tntl\": \"trying not to laugh\",\n",
    "        \"jk\": \"just kidding\",\n",
    "        \"idc\": \"i dont care\",\n",
    "        \"ily\": \"i love you\",\n",
    "        \"imu\": \"i miss you\",\n",
    "        \"zzz\": \"sleeping, bored, tired\",\n",
    "        \"ftw\": \"for the win\",\n",
    "        \"tbh\": \"to be honest\",\n",
    "        \"ftl\": \"for the loss\",\n",
    "        \"smh\": \"shaking my head\",\n",
    "        \"srsly\": \"seriously\",\n",
    "        \"afaik\": \"as far as i know\",\n",
    "        \"dm\": \"direct message\",\n",
    "        \"tldr\": \"too long didnt read\",\n",
    "        \"irl\": \"in real life\",\n",
    "        \"gl\": \"goodluck\",\n",
    "        \"ruok\": \"are you ok\",\n",
    "        \"w\": \"win\"\n",
    "    }\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(language))\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "    \n",
    "    def remove_special_characters(text):\n",
    "        # keep letters, numbers, underscores, colons (for demojized emojis)\n",
    "        text = re.sub('[^a-zA-Z0-9_]', ' ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def stem_text(tokens):\n",
    "        return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    def remove_stopwords_func(tokens):\n",
    "        return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    def remove_url_func(text):\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    def expand_slang(text):\n",
    "        words = text.split()\n",
    "        return \" \".join([slang_dict.get(w.lower(), w) for w in words])\n",
    "\n",
    "    # Clean process\n",
    "    text = contractions.fix(text)                        # fixing contraction\n",
    "\n",
    "    text = text.strip().lower()                          # lowercase + trim\n",
    "\n",
    "    if remove_url:\n",
    "        text = remove_url_func(text)                     # remove url\n",
    "    \n",
    "    # Handle emojis\n",
    "    if remove_emojis.lower() == \"remove\":\n",
    "        text = remove_special_characters(text)  # removes emojis\n",
    "\n",
    "    elif remove_emojis.lower() == \"convert\":\n",
    "        text = emoji.demojize(text, language=\"en\")  # e.g.,  -> ❤️ -> :red_heart:\n",
    "\n",
    "    elif remove_emojis.lower() == \"keep\":\n",
    "        pass\n",
    "\n",
    "    if expand_abbreviations:\n",
    "        text = remove_special_characters(text)   # <--- clean before slang expansion\n",
    "        text = expand_slang(text)\n",
    "\n",
    "    text = remove_special_characters(text)               # Remove other special characters (but preserve converted emojis with underscores)\n",
    "    \n",
    "    tokens = tokenize_text(text)                         # tokenize words\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens = remove_stopwords_func(tokens)           # remove stopwords\n",
    "        \n",
    "    if stem_words:\n",
    "        tokens = stem_text(tokens)                       # stemming\n",
    "\n",
    "    if tokenize:\n",
    "        return tokens                                    # return as tokens\n",
    "    else:\n",
    "        return \" \".join(tokens)                          # return as string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining_research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
